{"componentChunkName":"component---src-templates-post-template-tsx","path":"/gan-paper-memo/","result":{"data":{"markdown":{"frontmatter":{"slug":"/gan-paper-memo/","date":"2021.10.24","tags":["Deep Learning","Memo"],"title":"GAN関連の論文をいくつか読んだのでそのメモ","description":"最近GAN関連の論文をいくつか読んだので、備忘録としてまとめておきます。","thumbnail":{"name":"deeplearning-thumbnail","publicURL":"/static/8435363a9083f84955167155a35e14e9/deeplearning-thumbnail.png"}},"html":"<h2 id=\"これは何\" style=\"position:relative;\"><a href=\"#%E3%81%93%E3%82%8C%E3%81%AF%E4%BD%95\" aria-label=\"これは何 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>これは何</h2>\n<p>最近GAN関連の論文をいくつか読んだので、備忘録としてまとめておきます。</p>\n<h2 id=\"generative-adversarial-nets\" style=\"position:relative;\"><a href=\"#generative-adversarial-nets\" aria-label=\"generative adversarial nets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Nets</a></h2>\n<ul>\n<li>Tue, 10 Jun 2014</li>\n<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio</li>\n<li>最初のGAN。</li>\n<li>DとGは以下の価値関数で表されるminimaxゲームを行う。<br>\nmin_G max_D V (D, G) = Ex∼pdata(x) [log D(x)] + Ez∼pz(z) [log(1 − D(G(z)))].</li>\n<li>理論的には、DとGが十分なパラメータ空間を持ち、十分なトレーニング回数を確保出来るならば、Gはデータ生成分布を完全に再現可能で、Dは1/2の確率で正解を言い当てる状況に収束する。</li>\n<li>MNISTや、TFD、CIFER-10などのデータセットで学習した例が示されている。</li>\n<li>コード ... <a href=\"https://github.com/goodfeli/adversarial\">https://github.com/goodfeli/adversarial</a> (Theano)</li>\n</ul>\n<h2 id=\"conditional-generative-adversarial-nets\" style=\"position:relative;\"><a href=\"#conditional-generative-adversarial-nets\" aria-label=\"conditional generative adversarial nets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1411.1784\">Conditional Generative Adversarial Nets</a></h2>\n<ul>\n<li>Thu, 6 Nov 2014</li>\n<li>Mehdi Mirza, Simon Osindero</li>\n<li>GeneratorとDiscriminatorに、入力x以外にクラスラベルや他のモーダルからの入力yを供給することで条件付けを行う。</li>\n<li>検証として、MNISTデータセットに対してクラスラベルで条件付けをした例と、MIR Flickr 25,000 datasetのタグ付けを自動生成する例が示されていた。</li>\n<li>タグ付けをする例において、ImageNetで事前学習したモデルから取得した特徴量ベクトルと、Skip-gramから取得した埋め込みベクトルが使用されている。</li>\n<li>コード ... <a href=\"https://github.com/sverma88/Conditional-Generative-Adversarial-Networks\">https://github.com/sverma88/Conditional-Generative-Adversarial-Networks</a> (Theano)</li>\n</ul>\n<h2 id=\"unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks\" style=\"position:relative;\"><a href=\"#unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks\" aria-label=\"unsupervised representation learning with deep convolutional generative adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1511.06434\">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></h2>\n<ul>\n<li>Thu, 19 Nov 2015</li>\n<li>Alec Radford, Luke Metz, Soumith Chintala</li>\n<li>Deep Convolutional GANs (DCGAN)というCNNベースのGANを提案している。</li>\n<li>プーリング層をストライド畳み込み(Discriminator)とフラクショナルストライドコンボリューション(Generator)に置き換えている。</li>\n<li>ジェネレーターとディスクリミネーターの両方でBatchNormを使用している。</li>\n<li>より深いアーキテクチャのために、隠れ全結合層を削除している。</li>\n<li>GeneratorでTanhを使用する出力を除くすべてのレイヤーでReLUアクティベーションを使用している。</li>\n<li>DiscriminatorのすべてのレイヤーでLeakyReLUアクティベーションを使用している。</li>\n<li>コード ... <a href=\"https://github.com/floydhub/dcgan\">https://github.com/floydhub/dcgan</a> (PyTorch)</li>\n</ul>\n<h2 id=\"coupled-generative-adversarial-networks\" style=\"position:relative;\"><a href=\"#coupled-generative-adversarial-networks\" aria-label=\"coupled generative adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1606.07536\">Coupled Generative Adversarial Networks</a></h2>\n<ul>\n<li>Fri, 24 Jun 2016</li>\n<li>Ming-Yu Liu, Oncel Tuzel</li>\n<li>CoGANと呼ばれる、複数のドメインの画像を生成するGeneratorと、複数のドメインの画像を受け取るDiscriminatorによって構成されるGANを提案している。</li>\n<li>ペア付けがされていないマルチドメインの画像データを教師として、ペア付けがされたマルチドメインの画像データを生成出来るようになる。</li>\n<li>GeneratorとDiscriminatorの高次元の特徴量をデコードするレイヤーの重みを共有することで、ペア付けを可能にしている。</li>\n<li>Discriminatorの重みの共有は必須ではない。</li>\n<li>MNISTデータセット使ってdigit画像とそのネガティブ画像を生成したり、同一人物の顔の画像で、ブロンズヘアーバージョンや、スマイルバージョンや、眼鏡ありバージョンなどの画像を生成したり、RGB画像とそれに対応するdepth画像を生成したりしている。</li>\n<li>コード ... <a href=\"https://github.com/mingyuliutw/CoGAN\">https://github.com/mingyuliutw/CoGAN</a> (Caffe,PyTorch)</li>\n</ul>\n<h2 id=\"image-to-image-translation-with-conditional-adversarial-networks\" style=\"position:relative;\"><a href=\"#image-to-image-translation-with-conditional-adversarial-networks\" aria-label=\"image to image translation with conditional adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1611.07004\">Image-to-Image Translation with Conditional Adversarial Networks</a></h2>\n<ul>\n<li>Mon, 21 Nov 2016</li>\n<li>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros</li>\n<li>画像と画像のペアを教師データとして学習することで、ある画像から別の画像へのマッピングを可能にした。</li>\n<li>Generatorとして、セマンティックセグメンテーションとかで使われるU-Netを採用、DiscriminatorとしてPatchGAN(パッチごとのCNNを行う)を採用している。</li>\n<li>Generatorの損失関数として、Discriminatorのfakeかrealか以外にも追加でL1-lossを採用している。</li>\n<li>Discriminatorのfakeかrealかだけだとパターン化した人工模様が生成され、L1-lossだけだとぼやけて全体的に灰色な画像になってしまう。</li>\n<li>両者を足し合わせた損失関数を採用することで、シャープなエッジのカラフルな人工模様のない画像を生成することに成功している。</li>\n<li>コード ... <a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a> (PyTorch)</li>\n</ul>\n<h2 id=\"wasserstein-gan\" style=\"position:relative;\"><a href=\"#wasserstein-gan\" aria-label=\"wasserstein gan permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1701.07875\">Wasserstein GAN</a></h2>\n<ul>\n<li>Thu, 26 Jan 2017</li>\n<li>Martin Arjovsky, Soumith Chintala, Léon Bottou</li>\n<li>従来のGANではJensen-Shannon divergenceを使った損失関数を最適化していたが、新しくWasserstein距離を使った損失関数を最適化する手法を提案した。</li>\n<li>Kullback–Leibler divergence ... カルバック- ライブラー情報量。二つの確率分布の差異を測る尺度。</li>\n<li>Earth Mover Distance (EMD) ... ユークリッド距離のような距離尺度の1つ。Wassertstein距離と等価。</li>\n<li>従来のGANの最適化には、勾配消失、損失関数の値と生成されるサンプルの質に相関がないなどの問題を抱えていたが、Wasserstein距離を用いた最適化では、勾配消失が起こりにくく、また損失関数の値と生成されるサンプルの質に相関がある。</li>\n<li>WGANではCriticを十分に最適化したあとに、Generatorを最適化するため、GeneratorとDiscriminatorの最適化のバランスとかを考える必要がない。</li>\n<li>Criticのoptimizerには、Adamなどのmomentumベースのものではなく、RMSPropといったものを使う方が良い。</li>\n<li>コード ... <a href=\"https://github.com/eriklindernoren/PyTorch-GAN\">https://github.com/eriklindernoren/PyTorch-GAN</a> (PyTorch)\nまたは <a href=\"https://github.com/eriklindernoren/Keras-GAN\">https://github.com/eriklindernoren/Keras-GAN</a> (Tensorflow)</li>\n</ul>\n<h2 id=\"unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks\" style=\"position:relative;\"><a href=\"#unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks\" aria-label=\"unpaired image to image translation using cycle consistent adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1703.10593\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></h2>\n<ul>\n<li>Thu, 30 Mar 2017</li>\n<li>Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros</li>\n<li>CoGANのように、対になっていない二つのドメインの画像データを学習セットとして、一方のドメインの画像から他方のドメインの画像を生成するモデルを生成出来る。</li>\n<li>損失関数として、従来の敵対的損失(Adversarial Loss)以外に新しくサイクル一貫性損失(Cycle Consistency Loss)というものを導入している。</li>\n<li>二つのドメインX, Yに対して、マッピングG: X -> Y、F: Y -> Xがあった時に、サイクル一貫性損失関数は <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>−</mo><mi>X</mi><mi mathvariant=\"normal\">∣</mi><msub><mi mathvariant=\"normal\">∣</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">||F(G(X)) - X ||_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">))</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> や <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>G</mi><mo stretchy=\"false\">(</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>−</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><msub><mi mathvariant=\"normal\">∣</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">||G(F(Y)) - Y||_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣∣</span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mclose\">))</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> を小さくする要因として機能する。</li>\n<li>サイクル一貫性損失として、L1ノルムを用いるのではなく、他のDiscriminatorを使った場合も試してみたが性能向上は認められなかったらしい。</li>\n<li>スタイル変換、ラベルマップ-写真変換、オブジェクト変換など様々なタスクにおいて、CycleGANは他のベースラインモデルの性能を上回り、監視あり学習モデルであるpix2pixに匹敵する性能を示した。</li>\n<li>欠点として、単純な色やテクスチャの変換についてはほとんどのケースで上手くいっているが、犬 -> 猫などのジオメトリの変更が必要なケースでは変更を最小限に留めようとして上手く変換出来ないケースが多かったらしい。</li>\n<li>コード ... <a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a> (PyTorch)</li>\n</ul>\n<h2 id=\"stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks\" style=\"position:relative;\"><a href=\"#stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks\" aria-label=\"stackgan text to photo realistic image synthesis with stacked generative adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1612.03242\">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a></h2>\n<ul>\n<li>Sat, 10 Dec 2016</li>\n<li>Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas</li>\n<li>テキストを条件付けとして、対応する画像を生成するStackGANというモデルを提唱している。</li>\n<li>大まかな色と形状をスケッチするStage-Iと、高解像度でよりディティールの細かい画像を生成するStage-IIの二つのGANをstackしている。</li>\n<li>文字列の埋め込みベクトルから潜在変数への変換に、Conditioning Augmentationという方法を用いている。</li>\n<li>それまでの埋め込みベクトルから潜在変数への変換は、単純に非線形変換するだけだったが、そこにランダム性を持たせることによって、条件空間の小さな摂動に対してより堅牢になり、また同じ文字列でも様々なポーズの画像が生成出来るようになった。</li>\n<li>また、過去のモデルや、StackGANの様々な構成要素を比較する際の評価指標としてinception scoreというものを使っている。</li>\n<li>inception scoreは、inceptionモデルが識別しやすいほど、また識別されるラベルのバリエーションが多いほど高くなる。</li>\n<li>コード ... <a href=\"https://github.com/hanzhanggit/StackGAN\">https://github.com/hanzhanggit/StackGAN</a> (Tensorflow) または <a href=\"https://github.com/hanzhanggit/StackGAN-Pytorch\">https://github.com/hanzhanggit/StackGAN-Pytorch</a> (PyTorch)</li>\n</ul>\n<h2 id=\"progressive-growing-of-gans-for-improved-quality-stability-and-variation\" style=\"position:relative;\"><a href=\"#progressive-growing-of-gans-for-improved-quality-stability-and-variation\" aria-label=\"progressive growing of gans for improved quality stability and variation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1710.10196\">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a></h2>\n<ul>\n<li>Fri, 27 Oct 2017</li>\n<li>Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen</li>\n<li>低解像度の画像から学習していき、徐々にGeneratorとDiscriminatorの両方に対してレイヤーを加えて、高解像度の画像を生成する学習方法によって、生成される画像の質も、収束に要する時間も大幅に改善した。</li>\n<li>progressive growing以外にも、以下に挙げるような細かい改善や変更に対して評価を行っている。</li>\n<li>Small minibatch ... 高解像度の画像を用いて学習を行う場合は、メモリ制約によりバッチサイズが小さくなってしまうので、その影響を評価している。</li>\n<li>Revised training parameters ... 何を指しているのかよくわからなかったが、レイヤーを追加する時に、特徴量マップの補間を取って段々と移行することだと思っている。</li>\n<li>Minibatch discrimination ... あんましよく分かんなかった。なんか以前に提案された方法も試してみました的なことだと思っている。</li>\n<li>Minibatch stddev ... ミニバッチごとに標準偏差を取って、特徴量マップを追加する。</li>\n<li>Equalized learning rate ... 重みを単純に標準正規分布で初期化して、実行時にレイヤーごとに正規化するという方法。</li>\n<li>Pixelwise normalization ... 特徴量マップをピクセルごとに正規化する。</li>\n<li>Small minibatchでは全ての評価基準において精度が大幅に劣化したが、それ以外ではおおよそ精度の向上が見られた。</li>\n<li>progressive growingでは、学習の初期ではネットワークを流れる画像の解像度が小さいため、従来の方法に比べて、収束も速く、またスループットも大きい。</li>\n<li>損失関数は、WGAN-GP(一部LSGAN)を使用している。</li>\n<li>コード ... <a href=\"https://github.com/tkarras/progressive_growing_of_gans\">https://github.com/tkarras/progressive_growing_of_gans</a> (Tensorflow)</li>\n</ul>\n<h2 id=\"self-attention-generative-adversarial-networks\" style=\"position:relative;\"><a href=\"#self-attention-generative-adversarial-networks\" aria-label=\"self attention generative adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1805.08318\">Self-Attention Generative Adversarial Networks</a></h2>\n<ul>\n<li>Mon, 21 May 2018</li>\n<li>Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena</li>\n<li>GANにself-attention機構を導入した。</li>\n<li>frechet inception distance ... データセットの画像と生成した画像がそれぞれ多変量正規分布に従うと仮定した場合の、二つの分布関数を距離をとったもの。</li>\n<li>Spectral normalization ... DiscriminatorのBatch NormalizationをSpectral Normalizationに置き換えることで、WGANやWGAN-GPで前提としているようなリプシッツ制約を満たし、GANの安定性が向上させたらしい。元々はPFNの人が発表したらしい。</li>\n<li>TTUR ... Discriminatorの学習率をGeneratorの学習率よりも大きくすることで、それぞれ同じ頻度で学習しつつ、Discriminatorの学習をより速くすることに成功した。</li>\n<li>損失関数はヒンジ関数を用いている。</li>\n<li>コード ... <a href=\"https://github.com/brain-research/self-attention-gan\">https://github.com/brain-research/self-attention-gan</a> (Tensorflow)</li>\n</ul>\n<h2 id=\"large-scale-gan-training-for-high-fidelity-natural-image-synthesis\" style=\"position:relative;\"><a href=\"#large-scale-gan-training-for-high-fidelity-natural-image-synthesis\" aria-label=\"large scale gan training for high fidelity natural image synthesis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1809.11096\">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a></h2>\n<ul>\n<li>Fri, 28 Sep 2018</li>\n<li>Andrew Brock, Jeff Donahue, Karen Simonyan</li>\n<li>バッチサイズとモデルのサイズを大きくすることで、めっちゃ性能が向上した。</li>\n<li>Shared Embedding、Hierarchical latent spaces、Orthogonal Regularizationという制約を導入した。</li>\n<li>Shared Embedding ... 各層に導入されるconditional BatchNormの重みを共有する。</li>\n<li>conditional BatchNorm ... Bach Normalizationの学習パラメータである平均と分散にクラスラベルをMLPに通して組み込んだもの。</li>\n<li>Hierarchical latent spaces ... 複数の階層に対して、ランダムに生成されたノイズを加える。</li>\n<li>Orthogonal Regularization ... モデルの階層が深くなると、ノイズの潜在空間に対してGeneratorが滑らかでななくなり、truncation trickのために学習時と異なる分布のノイズが渡されたときに、不自然な画像を生成してしまう問題があった。Orthogonal Regularizationという制約を課すことにより、Generatorが滑らかになりやすくなるらしい。</li>\n<li>truncation trickという、Generatorに渡される乱数に対して制限を課す手法によって、生成される画像の忠実性と多様性を細かく制御出来るようにした。</li>\n<li>Projection Discriminator ... Discriminatorにクラスラベルを導入する時に、チャンネル方向に結合したり、class lossをとったりするのではなく、xの特徴量とクラスラベルの埋め込みベクトルの内積をとっている。なんでそれが上手くいくのかはちゃんと理解していない。</li>\n<li>progressive growingは使ってない。</li>\n<li>コード ... <a href=\"https://tfhub.dev/s?q=biggan\">https://tfhub.dev/s?q=biggan</a> (Tensorflow)</li>\n</ul>\n<h2 id=\"a-style-based-generator-architecture-for-generative-adversarial-networks\" style=\"position:relative;\"><a href=\"#a-style-based-generator-architecture-for-generative-adversarial-networks\" aria-label=\"a style based generator architecture for generative adversarial networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://arxiv.org/abs/1812.04948\">A Style-Based Generator Architecture for Generative Adversarial Networks</a></h2>\n<ul>\n<li>Wed, 12 Dec 2018</li>\n<li>Tero Karras, Samuli Laine, Timo Aila</li>\n<li>従来はランダムに生成された乱数はネットワークの最初の層のみに注入していたが、StyleGANではMapping Networkで変換した特徴量を各解像度で注入している。</li>\n<li>adaptive instance normalization(AdaIN) ... 各特徴量の平均と標準偏差を調整する。</li>\n<li>mixing regularization ... トレーニング中にある一定の確率で、二つの乱数から生成されたスタイル埋め込みベクトルw1の前半ととw2の後半をmixすることにより、各解像度でのスタイルの独立性を高めている。</li>\n<li>各解像度でノイズを注入することで、ネットワークが疑似乱数を生成する必要がなくなり、ネットワークのキャパシティが増えるらしい。</li>\n<li>PPL ... Perceptual path length。潜在空間での距離と知覚的な距離との比を表す。この値が小さいほど潜在空間でスタイルが線形的にとぎほぐされている。</li>\n<li>コード ... <a href=\"https://github.com/NVlabs/stylegan\">https://github.com/NVlabs/stylegan</a> (Tensorflow)</li>\n</ul>","toc":"<ul>\n<li><a href=\"#%E3%81%93%E3%82%8C%E3%81%AF%E4%BD%95\">これは何</a></li>\n<li><a href=\"#generative-adversarial-nets\">Generative Adversarial Nets</a></li>\n<li><a href=\"#conditional-generative-adversarial-nets\">Conditional Generative Adversarial Nets</a></li>\n<li><a href=\"#unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks\">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></li>\n<li><a href=\"#coupled-generative-adversarial-networks\">Coupled Generative Adversarial Networks</a></li>\n<li><a href=\"#image-to-image-translation-with-conditional-adversarial-networks\">Image-to-Image Translation with Conditional Adversarial Networks</a></li>\n<li><a href=\"#wasserstein-gan\">Wasserstein GAN</a></li>\n<li><a href=\"#unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></li>\n<li><a href=\"#stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks\">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a></li>\n<li><a href=\"#progressive-growing-of-gans-for-improved-quality-stability-and-variation\">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a></li>\n<li><a href=\"#self-attention-generative-adversarial-networks\">Self-Attention Generative Adversarial Networks</a></li>\n<li><a href=\"#large-scale-gan-training-for-high-fidelity-natural-image-synthesis\">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a></li>\n<li><a href=\"#a-style-based-generator-architecture-for-generative-adversarial-networks\">A Style-Based Generator Architecture for Generative Adversarial Networks</a></li>\n</ul>"},"site":{"siteMetadata":{"siteUrl":"https://www.takigawa-memo.com"}}},"pageContext":{"slug":"/gan-paper-memo/"}},"staticQueryHashes":["112949366","706912905"]}